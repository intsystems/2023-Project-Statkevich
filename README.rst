|test| |codecov| |docs|

.. |test| image:: https://github.com/intsystems/ProjectTemplate/workflows/test/badge.svg
    :target: https://github.com/intsystems/ProjectTemplate/tree/master
    :alt: Test status
    
.. |codecov| image:: https://img.shields.io/codecov/c/github/intsystems/ProjectTemplate/master
    :target: https://app.codecov.io/gh/intsystems/ProjectTemplate
    :alt: Test coverage
    
.. |docs| image:: https://github.com/intsystems/ProjectTemplate/workflows/docs/badge.svg
    :target: https://intsystems.github.io/ProjectTemplate/
    :alt: Docs status


.. class:: center

    :Problem: Methods with preconditioning with weight decay regularization.
    :Work type: M1P
    :Author: Статкевич Екатерина Игоревна
    :Scientific advisor: Безносиков Александр

Abstract
========

This work considers a regularization for such algorithms as Adam, OASIS for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The main difference from gradient descent is that Adam's and OASIS algorithm use information about previous gradients to update parameters of the model. 

Supplementations
================
1. `Link review <https://docs.google.com/document/d/1im8zvwoDYq_3vtAg8KPysuXejV8MWR5zGIJ86DTluvA/edit?usp=sharing>`_.
